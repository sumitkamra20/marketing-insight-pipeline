# Marketing Insight Pipeline - Capstone Project
**A Complete End-to-End Data Engineering & Machine Learning Pipeline**

## üéØ Project Overview

This capstone project demonstrates a **production-grade data engineering pipeline** that combines:
- **Batch Processing**: Historical sales and customer analytics using dbt
- **Real-time Streaming**: Live market data via Kafka and APIs
- **Machine Learning**: Customer segmentation with K-means clustering
- **Data Quality**: Comprehensive testing with 60+ automated tests
- **CI/CD**: GitHub Actions and dbt Cloud orchestration

**Technology Stack**: Snowflake, dbt, Kafka, Python, Docker, GitHub Actions, Jupyter Notebooks

---

## üèóÔ∏è Architecture Overview

### Batch Processing Pipeline
```mermaid
graph LR
    subgraph "Data Sources"
        CSV[Kaggle CSV Files<br/>customers, sales, marketing]
        ML[ML Pipeline<br/>Customer Segments CSV]
    end

    subgraph "RAW Schema"
        RAW_CUST[customers]
        RAW_SALES[online_sales]
        RAW_MARKET[marketing_spend]
    end

    subgraph "Analytics Schema"
        subgraph "Staging Layer"
            STG_CUST[stg_customers]
            STG_SALES[stg_online_sales]
            STG_MARKET[stg_marketing_spend]
        end

        subgraph "Dimensional Layer"
            DIM_CUST[dim_customer]
            DIM_PROD[dim_products]
            DIM_DATE[dim_datetime]
            FCT_SALES[fct_sales]
            FCT_SEGMENTS[fct_customer_segments]
        end

        subgraph "Seeds"
            SEED_SEG[customer_segments]
        end
    end

    CSV --> RAW_CUST
    CSV --> RAW_SALES
    CSV --> RAW_MARKET
    ML --> SEED_SEG

    RAW_CUST --> STG_CUST
    RAW_SALES --> STG_SALES
    RAW_MARKET --> STG_MARKET

    STG_CUST --> DIM_CUST
    STG_SALES --> DIM_PROD
    STG_SALES --> DIM_DATE
    STG_SALES --> FCT_SALES

    SEED_SEG --> FCT_SEGMENTS
    DIM_CUST --> FCT_SEGMENTS
    FCT_SALES --> FCT_SEGMENTS
```

### Real-Time Streaming Pipeline
```mermaid
graph LR
    subgraph "Live APIs"
        API1[CoinGecko API<br/>Bitcoin Prices<br/>30s intervals]
        API2[NewsAPI<br/>Market News<br/>60s intervals]
    end

    subgraph "Kafka Infrastructure"
        KAFKA[Kafka Cluster<br/>Topics: bitcoin-prices<br/>news-events]
    end

    subgraph "Streaming Schema"
        BTC_RAW[bitcoin_prices_raw]
        NEWS_RAW[news_events_raw]
    end

    subgraph "dbt Cloud Orchestration"
        SCHED[Hourly Scheduled Job<br/>Streaming Data Materialization]
    end

    subgraph "Streaming Analytics"
        STG_BTC[stg_bitcoin<br/>INCREMENTAL MODEL]
        STG_NEWS[stg_news<br/>INCREMENTAL MODEL]
    end

    API1 --> KAFKA
    API2 --> KAFKA
    KAFKA --> BTC_RAW
    KAFKA --> NEWS_RAW
    BTC_RAW --> SCHED
    NEWS_RAW --> SCHED
    SCHED --> STG_BTC
    SCHED --> STG_NEWS
```

---

## üìä Enhanced Data Model with ML Integration

### Star Schema with Customer Segmentation

```mermaid
erDiagram
    dim_customer {
        string customer_id PK "Primary Key"
        string gender
        string location
        int customer_tenure_months
        string customer_segment
        decimal customer_tenure_years
    }

    dim_products {
        string product_sku PK "Primary Key"
        string category
        decimal gst_rate
        string product_group
        string product_name
    }

    dim_datetime {
        date date_day PK "Primary Key"
        int year
        int month
        int day
        string month_name
        boolean is_weekend
        int quarter
    }

    fct_sales {
        string transaction_id PK "Primary Key"
        string customer_id FK "Foreign Key to dim_customer"
        string product_sku FK "Foreign Key to dim_products"
        date transaction_date FK "Foreign Key to dim_datetime"
        int quantity
        decimal avg_price
        decimal gross_sales_amount
        decimal discount_amount
        decimal gst_amount
        decimal total_amount
        string sale_size_category
        timestamp processed_at
    }

    customer_segments {
        string customer_id PK "Primary Key - Seed Table"
        int segment_id "ML Generated Segment ID (0-4)"
        string segment_name "Business Segment Name"
    }

    fct_customer_segments {
        string customer_id PK "Primary Key"
        string location
        string gender
        int customer_tenure_months
        int segment_id "ML Segment ID"
        string segment_name "ML Segment Name"
        int total_orders
        decimal total_revenue
        date first_purchase_date
        date last_purchase_date
        int days_since_last_purchase
        decimal avg_order_value
        string activity_status
        timestamp updated_at
    }

    dim_customer ||--o{ fct_sales : "One customer to many sales"
    dim_products ||--o{ fct_sales : "One product to many sales"
    dim_datetime ||--o{ fct_sales : "One date to many sales"

    customer_segments ||--|| fct_customer_segments : "ML Segments to Customer Facts"
    dim_customer ||--|| fct_customer_segments : "Customer Profile to Segments"
    fct_sales ||--o{ fct_customer_segments : "Sales aggregated by customer"
```

### ML Customer Segments Defined
| Segment ID | Segment Name | Business Description |
|------------|--------------|---------------------|
| 0 | Big Spenders | High-value customers with large transaction amounts |
| 1 | At-Risk (Lapsing) | Previously active customers showing decline |
| 2 | Occasional Shoppers | Regular but moderate purchasing behavior |
| 3 | Champions (VIPs) | Top-tier customers with high value and frequency |
| 4 | Loyal Customers | Consistent, regular customers with good retention |

---

## üöÄ Live Demo Commands

### 1. üìà Incremental Models Demo (Streaming Data)
**Shows real-time data processing with incremental materialization**

```bash
cd dbt_pipeline

# Show current streaming data count
echo "=== Current Streaming Data Count ==="
dbt run-operation get_row_count --args '{table_name: "stg_bitcoin"}'

# Run incremental models (only processes new data)
echo "=== Running Incremental Streaming Models ==="
dbt run --select tag:streaming

# Show updated count (should be higher)
echo "=== Updated Streaming Data Count ==="
dbt run-operation get_row_count --args '{table_name: "stg_bitcoin"}'

# Show incremental strategy in action
echo "=== Showing Incremental Logic ==="
dbt show --select stg_bitcoin --limit 5
```

### 2. ‚úÖ Comprehensive Testing Demo
**Demonstrates both generic and singular tests**

#### Generic Tests (Built-in dbt tests):
- **Uniqueness**: Primary key constraints
- **Not Null**: Required field validation
- **Relationships**: Foreign key integrity
- **Accepted Values**: Enum validation

#### Singular Tests (Custom business logic):
- **Valid Transaction Amount**: Business rule validation
- **Customer Segment Validation**: ML segment consistency

```bash
cd dbt_pipeline

# Run all tests with detailed output
echo "=== Running All 60+ Data Quality Tests ==="
dbt test --store-failures

# Run specific test categories
echo "=== Generic Tests (Built-in) ==="
dbt test --select test_type:generic

echo "=== Singular Tests (Custom Business Logic) ==="
dbt test --select test_type:singular

# Show test failures (if any) for debugging
echo "=== Test Results Summary ==="
dbt test --select fct_sales --store-failures-as table
```

### 3. üîß Custom Generic Test Demo
**Shows custom `valid_transaction_amount` test in action**

#### **Implementation Details:**
- **File**: `macros/test_valid_transaction_amount.sql`
- **Applied to**: `fct_sales.total_amount` column in `models/marts/schema.yml`
- **Business Rule**: Transaction amounts between $0-$15,000
- **Configuration**: `severity: warn` (warns but doesn't fail build)

```yaml
# In models/marts/schema.yml
- name: total_amount
  description: "Final transaction amount"
  tests:
    - valid_transaction_amount:
        min_amount: 0
        max_amount: 15000
        config:
          severity: warn
```

#### **Test Logic:**
```sql
-- Returns count of invalid records (test passes if count = 0)
select count(*)
from {{ model }}
where {{ column_name }} < {{ min_amount }}
   or {{ column_name }} > {{ max_amount }}
   or {{ column_name }} is null
```

```bash
cd dbt_pipeline

# Run the custom generic test
echo "=== Custom Generic Test: Valid Transaction Amount ==="
dbt test --select test_name:valid_transaction_amount

# Show test definition
echo "=== Custom Test Implementation ==="
cat macros/test_valid_transaction_amount.sql

# Test with edge cases
echo "=== Running Custom Test with Warnings ==="
dbt test --select valid_transaction_amount --warn-error

# Show which records would fail (if any)
echo "=== Show Invalid Records (Demo Purpose) ==="
dbt test --select fct_sales:total_amount --store-failures
```

### 4. üî® Custom Macro Demo
**Shows reusable SQL logic with `categorize_sale_size` and `get_current_timestamp`**

#### **Implementation Details:**
- **File**: `macros/calculate_revenue_metrics.sql`
- **Used in**: `models/marts/fct_sales.sql` (lines 86 & 89)
- **Purpose**: Business logic reuse and audit trail

#### **Macro 1: `categorize_sale_size`**
```sql
# In fct_sales.sql (line 86):
{{ categorize_sale_size('(s.quantity * s.avg_price)') }} as sale_size_category,

# Generates:
case
    when (s.quantity * s.avg_price) < 50 then 'Small'
    when (s.quantity * s.avg_price) between 50 and 200 then 'Medium'
    when (s.quantity * s.avg_price) between 201 and 500 then 'Large'
    else 'Extra Large'
end as sale_size_category
```

#### **Macro 2: `get_current_timestamp`**
```sql
# In fct_sales.sql (line 89):
{{ get_current_timestamp() }} as processed_at

# Generates:
current_timestamp() as processed_at
```

```bash
cd dbt_pipeline

# Show macro definitions
echo "=== Custom Macros Implementation ==="
cat macros/calculate_revenue_metrics.sql

# Compile model to see macro expansion
echo "=== Macros in Action (Compiled SQL) ==="
dbt compile --select fct_sales
cat target/compiled/dbt_pipeline/models/marts/fct_sales.sql | grep -A 5 -B 2 "case.*Small\|current_timestamp"

# Run model using macros
echo "=== Running Model with Custom Macros ==="
dbt run --select fct_sales

# Show macro results - Sale Categories
echo "=== Macro Output: Sale Size Categories ==="
dbt run-operation query --args "select sale_size_category, count(*) as transaction_count, round(avg(total_amount), 2) as avg_amount from {{ ref('fct_sales') }} group by 1 order by avg_amount"

# Show macro results - Processing Timestamps
echo "=== Macro Output: Processing Audit Trail ==="
dbt run-operation query --args "select min(processed_at) as first_processed, max(processed_at) as last_processed, count(*) as total_records from {{ ref('fct_sales') }}"
```

### 5. üì∏ dbt Snapshot Demo (SCD Type 2)
**Demonstrates Slowly Changing Dimensions tracking**

```bash
cd dbt_pipeline

# First, take initial snapshot
echo "=== Taking Initial Customer Snapshot ==="
dbt snapshot

# Show current snapshot data
echo "=== Current Snapshot State ==="
dbt run-operation query --args "select customer_id, gender, location, dbt_valid_from, dbt_valid_to from {{ ref('customer_snapshot') }} where customer_id = '12347' order by dbt_valid_from"

# Simulate customer data change
echo "=== Simulating Customer Data Change ==="
dbt run-operation query --args "
  update MARKETING_INSIGHTS_DB.RAW.customers
  set location = 'Mumbai'
  where customerid = '12347'"

# Take new snapshot to capture change
echo "=== Taking Updated Snapshot (SCD Type 2) ==="
dbt snapshot

# Show SCD Type 2 result - customer now has 2 records
echo "=== SCD Type 2 Result: Customer History Tracked ==="
dbt run-operation query --args "
  select
    customer_id,
    location,
    dbt_valid_from,
    dbt_valid_to,
    case when dbt_valid_to is null then 'CURRENT' else 'HISTORICAL' end as record_status
  from {{ ref('customer_snapshot') }}
  where customer_id = '12347'
  order by dbt_valid_from"
```

### 6. ü§ñ Machine Learning Integration Demo
**Shows ML model integration with dbt using seeds**

#### ML Pipeline Overview:
- **Algorithm**: K-means clustering with optimal K=5
- **Features**: RFM Analysis (Recency, Frequency, Monetary + derived features)
- **Data**: 52,900+ customer transactions
- **Output**: 1,468 customers segmented into 5 business-meaningful groups

**Jupyter Notebook**: [Customer Segmentation ML Pipeline](ml_pipeline/notebooks/customer_segmentation.ipynb)

```bash
cd dbt_pipeline

# Load ML-generated customer segments into dbt
echo "=== Loading ML Customer Segments via dbt Seed ==="
dbt seed --select customer_segments

# Create business mart combining ML segments with customer data
echo "=== Building Customer Segmentation Mart ==="
dbt run --select fct_customer_segments

# Validate ML integration
echo "=== Testing ML Integration ==="
dbt test --select customer_segments fct_customer_segments

# Show ML segmentation results
echo "=== ML Customer Segmentation Results ==="
dbt run-operation query --args "
  select
    segment_name,
    count(*) as customers,
    round(avg(total_revenue), 2) as avg_revenue,
    round(sum(total_revenue), 2) as total_segment_revenue
  from {{ ref('fct_customer_segments') }}
  group by segment_name
  order by total_segment_revenue desc"

# Show specific segment insights
echo "=== At-Risk Customers for Retention Campaigns ==="
dbt run-operation query --args "
  select customer_id, total_revenue, days_since_last_purchase
  from {{ ref('fct_customer_segments') }}
  where segment_name = 'At-Risk (Lapsing)'
    and total_revenue > 1000
  order by total_revenue desc
  limit 10"
```

---

## üé¨ Complete Demo Workflow

### Full Pipeline Demonstration (5-10 minutes)

```bash
cd dbt_pipeline

echo "üöÄ === MARKETING INSIGHT PIPELINE DEMO ==="

echo "1Ô∏è‚É£ === Testing dbt Connection ==="
dbt debug

echo "2Ô∏è‚É£ === Installing Dependencies ==="
dbt deps

echo "3Ô∏è‚É£ === Running All Models (12 models) ==="
dbt run

echo "4Ô∏è‚É£ === Running All Tests (60+ tests) ==="
dbt test

echo "5Ô∏è‚É£ === Demonstrating Incremental Models ==="
dbt run --select tag:streaming

echo "6Ô∏è‚É£ === Custom Generic Test ==="
dbt test --select test_name:valid_transaction_amount

echo "7Ô∏è‚É£ === dbt Snapshot (SCD Type 2) ==="
dbt snapshot

echo "8Ô∏è‚É£ === ML Customer Segmentation ==="
dbt run --select fct_customer_segments
dbt test --select fct_customer_segments

echo "9Ô∏è‚É£ === Generating Documentation ==="
dbt docs generate
echo "üìñ Open http://localhost:8080 to view data lineage"
dbt docs serve --port 8080

echo "‚úÖ === DEMO COMPLETE ==="
```

---

---

## üéØ **Advanced dbt Features - Implementation Summary**

### üîß **Custom Generic Tests**
| Feature | File | Applied To | Purpose |
|---------|------|------------|---------|
| `valid_transaction_amount` | `macros/test_valid_transaction_amount.sql` | `fct_sales.total_amount` | Validates business rules: $0-$15K range |

### üî® **Custom Macros**
| Macro | File | Used In | Line | Purpose |
|-------|------|---------|------|---------|
| `categorize_sale_size` | `macros/calculate_revenue_metrics.sql` | `fct_sales.sql` | 86 | Categorizes transactions: Small/Medium/Large/Extra Large |
| `get_current_timestamp` | `macros/calculate_revenue_metrics.sql` | `fct_sales.sql` | 89 | Adds audit timestamp for data lineage |

### üì∏ **SCD Type 2 Snapshots**
| Feature | File | Target Schema | Strategy | Purpose |
|---------|------|---------------|----------|---------|
| `customer_history` | `snapshots/customer_history.sql` | `SNAPSHOTS` | `check_cols='all'` | Tracks all customer data changes over time |

### üìà **Incremental Models**
| Model | File | Strategy | Purpose |
|-------|------|----------|---------|
| `fct_sales` | `models/marts/fct_sales.sql` | `unique_key='transaction_id'` | Processes only new transactions |
| `stg_bitcoin` | `models/stream_stg/stg_bitcoin.sql` | `unique_key + timestamp` | Real-time Bitcoin price processing |
| `stg_news` | `models/stream_stg/stg_news.sql` | `unique_key + timestamp` | Real-time news event processing |

---

## üìà Key Metrics & Achievements

- **üìä Data Models**: 12 dbt models (5 staging, 4 marts, 2 streaming, 1 seed)
- **üß™ Data Quality**: 60+ automated tests with 100% pass rate
- **‚ö° Real-time Processing**: Sub-minute latency from API to analytics
- **ü§ñ ML Integration**: 1,468 customers segmented into 5 actionable groups
- **üîÑ CI/CD**: Automated testing and deployment via GitHub Actions
- **üìñ Documentation**: Complete data lineage and business logic documentation
- **üèóÔ∏è Architecture**: Production-ready with incremental processing and SCD tracking
- **üîß Advanced dbt**: Custom tests, macros, snapshots, and incremental models

## üõ†Ô∏è Technology Stack

- **Data Warehouse**: Snowflake
- **Transformation**: dbt (Data Build Tool)
- **Streaming**: Apache Kafka + Docker
- **ML**: Python, Pandas, Scikit-learn, Jupyter
- **Orchestration**: dbt Cloud
- **CI/CD**: GitHub Actions
- **APIs**: CoinGecko (Bitcoin), NewsAPI (Market News)

---

**üéØ This project demonstrates enterprise-grade data engineering skills with real-world applications in marketing analytics and customer intelligence.**
