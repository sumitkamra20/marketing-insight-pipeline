# ğŸš€ Bitcoin & News Streaming Pipeline

Simple **real-time streaming** from Bitcoin prices and news headlines to Snowflake via Kafka.

## ğŸ“Š Data Sources

### âœ… **No API Key Required (Works Immediately)**
- **CoinGecko API**: Real-time Bitcoin prices & 24hr changes

### ğŸ”‘ **Free API Key (Optional)**
- **NewsAPI**: Live business news headlines (1,000 calls/day free)

## ğŸ—ï¸ Architecture

```
Bitcoin API â†’ Producer â†’ Kafka Topics â†’ Consumer â†’ Snowflake Tables
News API         â†“           â†“            â†“              â†“
    â†“         JSON       2 Topics    Real-time      STREAMING
    â†“        Events      bitcoin     Processing     Schema
    â†“                    news                       bitcoin_prices
    â†“                                              news_headlines
    â””â”€â”€ Real-time data every 10 seconds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. Start Kafka Cluster
```bash
docker-compose up -d
```

**Verify at:** http://localhost:8080 (Kafka UI)

### 2. Set Up Environment
```bash
# Create virtual environment
python -m venv .env
source .env/bin/activate  # Windows: .env\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configure APIs (Optional)

**Get Free API Keys:**
- Weather: https://openweathermap.org/api (1,000 calls/day free)
- News: https://newsapi.org (1,000 requests/day free)

**Update `producer_simple.py`:**
```python
# Line 53: Replace with your NewsAPI key
api_key = "YOUR_NEWS_API_KEY"  # Get free at newsapi.org
```

### 4. Configure Snowflake

**Update `consumer_snowflake.py`:**
```python
self.snowflake_config = {
    'user': 'your_username',
    'password': 'your_password',
    'account': 'your_account',
    'warehouse': 'COMPUTE_WH',
    'database': 'MARKETING_INSIGHTS_DB'
}
```

### 5. Run the Pipeline

#### **Option A: Test with Console First (Recommended)**

**Terminal 1 - Start Console Consumer:**
```bash
python consumer_console.py
```

**Terminal 2 - Start Producer:**
```bash
python producer_simple.py
```

Verify data format and APIs working, then move to Snowflake:

#### **Option B: Full Pipeline with Snowflake**

**Terminal 1 - Start Snowflake Consumer:**
```bash
python consumer_snowflake.py
```

**Terminal 2 - Start Producer:**
```bash
python producer_simple.py
```

## ğŸ“‹ What You'll See

### Producer Logs:
```
ğŸš€ Starting simplified data producer...
ğŸ“¤ Bitcoin: $67,234.50 (+2.34%)
ğŸ“° News: Reuters
```

### Consumer Logs:
```
âœ… Connected to Snowflake
ğŸ’° Bitcoin data inserted: $67,234.50 (+2.34%)
ğŸ“° News data inserted: Reuters
```

### Snowflake Tables:
- `STREAMING.BITCOIN_PRICES` - Real Bitcoin prices & 24hr changes
- `STREAMING.NEWS_HEADLINES` - Live business news headlines

## ğŸ” Monitoring

- **Kafka UI**: http://localhost:8080
- **Topics**: `bitcoin`, `news`
- **Latency**: < 5 seconds from API to Snowflake

## âš¡ API Rate Limits & Frequency

- **Bitcoin API (CoinGecko)**: Unlimited, fetched every 30 seconds
- **News API**: 1,000 requests/day free tier, fetched every 90 seconds
- **Daily calculation**: 90-second intervals = 960 calls/day for 16 hours (within 1000 limit)
- **ğŸ’¡ Tip**: Turn off at night to stay comfortably within NewsAPI limits

## ğŸ“ˆ Demo Queries

```sql
-- Real-time Bitcoin prices
SELECT price, change_24h, event_timestamp
FROM STREAMING.BITCOIN_PRICES
ORDER BY ingestion_timestamp DESC LIMIT 10;

-- Latest business news
SELECT headline, source_name, published_at
FROM STREAMING.NEWS_HEADLINES
ORDER BY ingestion_timestamp DESC LIMIT 5;

-- Price trend analysis
SELECT
    DATE_TRUNC('hour', event_timestamp) as hour,
    AVG(price) as avg_price,
    AVG(change_24h) as avg_change
FROM STREAMING.BITCOIN_PRICES
GROUP BY hour
ORDER BY hour DESC;
```

## ğŸ›‘ Shutdown

```bash
# Stop producer/consumer (Ctrl+C)
# Stop Kafka cluster
docker-compose down
```

## âœ¨ Academic Value

Demonstrates:
- âœ… **Real-time streaming** (< 5 min latency)
- âœ… **Live external APIs** (Bitcoin prices + news)
- âœ… **Kafka topics & consumers**
- âœ… **Snowflake integration**
- âœ… **Separate tables** for different data types

Perfect for academic streaming pipeline demo! ğŸ¯
